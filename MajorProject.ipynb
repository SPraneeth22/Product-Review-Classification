{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Using Semantic Analysis"
      ],
      "metadata": {
        "id": "hHlleDt8w55T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from textblob import TextBlob\n",
        "import re"
      ],
      "metadata": {
        "id": "krbI8dOrs8Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXIIOLoOtC_B",
        "outputId": "9019ae89-406d-4a0d-d333-3346a43b8a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step1 : data preprocessing\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\" v\", \" very\", phrase)\n",
        "    return phrase\n",
        "\n",
        "reviewerID =[]\n",
        "productID = []\n",
        "reviewerName=[]\n",
        "liked_and_seen = []\n",
        "reviewText = []\n",
        "rating = []\n",
        "summary = []\n",
        "unixTime = []\n",
        "date = []\n",
        "\n",
        "with open('Cell_Phones_and_Accessories_5.json') as json_data:\n",
        "    d = json.load(json_data)\n",
        "    d = d[0:300]\n",
        "\n",
        "for i in range(len(d)):\n",
        "    reviewText.append(decontracted(d[i]['reviewText']))\n",
        "    rating.append(d[i]['overall'])\n",
        "    reviewerID.append(d[i]['reviewerID'])\n",
        "    productID.append(d[i]['asin'])\n",
        "#    reviewerName.append(d[i]['reviewerName'])\n",
        "    liked_and_seen.append(d[i]['helpful'])\n",
        "    summary.append(d[i]['summary'])\n",
        "    unixTime.append(d[i]['unixReviewTime'])\n",
        "    date.append(d[i]['reviewTime'])\n",
        "\n",
        "\n",
        "# create  dataset\n",
        "from pandas import DataFrame\n",
        "dataset = DataFrame({'reviewerID': reviewerID, 'productID': productID, 'liked_and_seen': liked_and_seen, 'reviewText': reviewText, 'rating': rating, 'summary': summary, 'unixTime': unixTime, 'date': date})\n",
        "\n",
        "#cleaning unwanted symbols\n",
        "#cleaning unwanted symbols\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "comment_dict = defaultdict(list)\n",
        "for i in range(len(dataset)):\n",
        "    sentence = re.sub('[^a-zA-Z.]',' ',dataset['reviewText'][i])\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.split('.')\n",
        "    for k in range(len(sentence)):\n",
        "        review = sentence[k].split()\n",
        "        review = [word for word in review if not word in set(stopwords.words('english'))]\n",
        "        sentence[k] =  ' '.join(review)\n",
        "        comment_dict[i].append(sentence[k])\n",
        "\n",
        "#delete unwanted '' words\n",
        "for j in range(len(comment_dict)):\n",
        "    comment_dict[j] = [comment_dict[j][i] for i in range(len(comment_dict[j])) if comment_dict[j][i] not in '']\n",
        "\n",
        "for i in range(len(comment_dict)):\n",
        "    reviewText[i] = ('. '.join(comment_dict[i][j] for j in range(len(comment_dict[i]))))\n",
        "\n",
        "# spelling correction\n",
        "for i in range(len(reviewText)):\n",
        "    b = TextBlob(reviewText[i])\n",
        "    reviewText[i] = b.correct()\n",
        "\n",
        "dataset_corrected= DataFrame({'reviewerID': reviewerID, 'productID': productID, 'liked_and_seen': liked_and_seen, 'reviewText': reviewText, 'rating': rating, 'summary': summary, 'unixTime': unixTime, 'date': date})\n",
        "\n",
        "# creating corpus\n",
        "corpus = defaultdict(set)\n",
        "for i in range(len(reviewText)):\n",
        "    wiki = reviewText[i]\n",
        "    corpus[i] = wiki.sentences\n",
        "\n",
        "corpus_key = corpus.keys()\n",
        "corpus_list = defaultdict(list)\n",
        "\n",
        "for i in corpus_key:\n",
        "    for j in range(len(corpus[i])):\n",
        "        word = ' '.join(corpus[i][j].words)\n",
        "        corpus_list[i].append(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd40yyN8tH7E",
        "outputId": "3bde523e-bd13-4035-8176-31ff21d2bd08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: adding biwords and triwords for generating patterns\n",
        "\n",
        "length = defaultdict(list)\n",
        "for i in corpus_key:\n",
        "    length[i] = list(corpus_list[i])\n",
        "\n",
        "# triwords\n",
        "for i in corpus_key:\n",
        "    for j in range(len(length[i])):\n",
        "        text = TextBlob(length[i][j])\n",
        "        text = text.ngrams(n=3)\n",
        "        for k in range(len(text)):\n",
        "            triword = [' '.join([text[k][l] for l in range(len(text[k]))])]\n",
        "            triword = triword[0]\n",
        "            corpus_list[i].append(triword)\n",
        "#biwords\n",
        "for i in corpus_key:\n",
        "    for j in range(len(length[i])):\n",
        "        text = TextBlob(length[i][j])\n",
        "        text = text.ngrams(n=2)\n",
        "        for k in range(len(text)):\n",
        "            triword = [' '.join([text[k][l] for l in range(len(text[k]))])]\n",
        "            triword = triword[0]\n",
        "            corpus_list[i].append(triword)"
      ],
      "metadata": {
        "id": "JhGnwpxktPdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Part-of-speech Tagging\n",
        "\n",
        "pos_dict = defaultdict(list)\n",
        "for i in corpus_key:\n",
        "    for j in range(len(corpus_list[i])):\n",
        "        text = TextBlob(corpus_list[i][j])\n",
        "        text = text.tags\n",
        "        pos_dict[i].append(text)\n",
        "\n",
        "pos_dict_key = pos_dict.keys()\n",
        "\n",
        "corpus_noun = defaultdict(list)\n",
        "for i in pos_dict_key:\n",
        "    for j in range(len(pos_dict[i])):\n",
        "        for k in range(len(pos_dict[i][j])):\n",
        "            if(pos_dict[i][j][k][1] == 'NN'):\n",
        "                corpus_noun[i].append(pos_dict[i][j][k])"
      ],
      "metadata": {
        "id": "qkfxd23Xtqn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: pattern generation from Part -of -speech tagging\n",
        "pattern1 =  defaultdict(list)\n",
        "for i in pos_dict_key:\n",
        "    for j in range(len(pos_dict[i])):\n",
        "        if(len(pos_dict[i][j]) == 2):\n",
        "            if((pos_dict[i][j][0][1] == 'JJ' and pos_dict[i][j][1][1] == 'NN') or (pos_dict[i][j][0][1] == 'JJ' and pos_dict[i][j][1][1] == 'NNS')):\n",
        "                #pattern1\n",
        "                pattern1[i].append(pos_dict[i][j])\n",
        "\n",
        "pattern2 =  defaultdict(list)\n",
        "for i in pos_dict_key:\n",
        "    for j in range(len(pos_dict[i])):\n",
        "        if(len(pos_dict[i][j]) == 3):\n",
        "            if((pos_dict[i][j][0][1] == 'JJ' and pos_dict[i][j][1][1] == 'NN'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'JJ' and pos_dict[i][j][1][1] == 'NN'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern2\n",
        "                pattern2[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'JJ' and pos_dict[i][j][1][1] == 'NNS'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'JJ' and pos_dict[i][j][1][1] == 'NNS'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern2\n",
        "                pattern2[i].append(pos_dict[i][j])\n",
        "\n",
        "pattern3 =  defaultdict(list)\n",
        "for i in pos_dict_key:\n",
        "    for j in range(len(pos_dict[i])):\n",
        "        if(len(pos_dict[i][j]) == 2):\n",
        "            if((pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'JJ') or (pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'JJ') or (pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'JJ')):\n",
        "                #pattern3\n",
        "                pattern3[i].append(pos_dict[i][j])\n",
        "\n",
        "pattern4 =  defaultdict(list)\n",
        "for i in pos_dict_key:\n",
        "    for j in range(len(pos_dict[i])):\n",
        "        if(len(pos_dict[i][j]) == 3):\n",
        "            if((pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'JJ'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'JJ'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RB'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RB'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBR'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBR'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBS'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBS'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'JJ'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'JJ'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'RB'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RB'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'RBR'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBR'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'RBS'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBS'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'JJ'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'JJ'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'RB'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RB'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'RBR'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBR'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'RBS'and pos_dict[i][j][2][1] == 'NN') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBS'and pos_dict[i][j][2][1] == 'NNS')):\n",
        "                #pattern4\n",
        "                pattern4[i].append(pos_dict[i][j])\n",
        "\n",
        "pattern5 =  defaultdict(list)\n",
        "for i in pos_dict_key:\n",
        "    for j in range(len(pos_dict[i])):\n",
        "        if(len(pos_dict[i][j]) == 2):\n",
        "            if((pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'VBN') or (pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'VBN') or (pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'VBN')):\n",
        "                #pattern5\n",
        "                pattern5[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'VBD') or (pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'VBD') or (pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'VBD')):\n",
        "                #pattern5\n",
        "                pattern5[i].append(pos_dict[i][j])\n",
        "\n",
        "pattern6 =  defaultdict(list)\n",
        "for i in pos_dict_key:\n",
        "    for j in range(len(pos_dict[i])):\n",
        "        if(len(pos_dict[i][j]) == 3):\n",
        "            if((pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RB'and pos_dict[i][j][2][1] == 'JJ') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBR'and pos_dict[i][j][2][1] == 'JJ') or (pos_dict[i][j][0][1] == 'RB' and pos_dict[i][j][1][1] == 'RBS'and pos_dict[i][j][0][1] == 'JJ')):\n",
        "                #pattern6\n",
        "                pattern6[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'RB'and pos_dict[i][j][2][1] == 'JJ') or (pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'RBR'and pos_dict[i][j][2][1] == 'JJ') or (pos_dict[i][j][0][1] == 'RBR' and pos_dict[i][j][1][1] == 'RBS'and pos_dict[i][j][0][1] == 'JJ')):\n",
        "                #pattern6\n",
        "                pattern6[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'RB'and pos_dict[i][j][2][1] == 'JJ') or (pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'RBR'and pos_dict[i][j][2][1] == 'JJ') or (pos_dict[i][j][0][1] == 'RBS' and pos_dict[i][j][1][1] == 'RBS'and pos_dict[i][j][0][1] == 'JJ')):\n",
        "                #pattern6\n",
        "                pattern6[i].append(pos_dict[i][j])\n",
        "\n",
        "pattern7 =  defaultdict(list)\n",
        "for i in pos_dict_key:\n",
        "    for j in range(len(pos_dict[i])):\n",
        "        if(len(pos_dict[i][j]) == 2):\n",
        "            if((pos_dict[i][j][0][1] == 'VBN' and pos_dict[i][j][1][1] == 'NN') or (pos_dict[i][j][0][1] == 'VBD' and pos_dict[i][j][1][1] == 'NN')):\n",
        "                #pattern7\n",
        "                pattern7[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'VBN' and pos_dict[i][j][1][1] == 'NNS') or (pos_dict[i][j][0][1] == 'VBD' and pos_dict[i][j][1][1] == 'NNS')):\n",
        "                #pattern7\n",
        "                pattern7[i].append(pos_dict[i][j])\n",
        "\n",
        "pattern8 =  defaultdict(list)\n",
        "for i in pos_dict_key:\n",
        "    for j in range(len(pos_dict[i])):\n",
        "        if(len(pos_dict[i][j]) == 2):\n",
        "            if((pos_dict[i][j][0][1] == 'VBN' and pos_dict[i][j][1][1] == 'RB') or (pos_dict[i][j][0][1] == 'VBD' and pos_dict[i][j][1][1] == 'RB')):\n",
        "                #pattern8\n",
        "                pattern8[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'VBN' and pos_dict[i][j][1][1] == 'RBR') or (pos_dict[i][j][0][1] == 'VBD' and pos_dict[i][j][1][1] == 'RBR')):\n",
        "                #pattern8\n",
        "                pattern8[i].append(pos_dict[i][j])\n",
        "            elif((pos_dict[i][j][0][1] == 'VBN' and pos_dict[i][j][1][1] == 'RBS') or (pos_dict[i][j][0][1] == 'VBD' and pos_dict[i][j][1][1] == 'RBS')):\n",
        "                #pattern8\n",
        "                pattern8[i].append(pos_dict[i][j])\n",
        "\n",
        "pattern = defaultdict(set)\n",
        "pattern.update(pattern1)\n",
        "pattern.update(pattern2)\n",
        "pattern.update(pattern3)\n",
        "pattern.update(pattern4)\n",
        "pattern.update(pattern5)\n",
        "pattern.update(pattern6)\n",
        "pattern.update(pattern7)\n",
        "pattern.update(pattern8)"
      ],
      "metadata": {
        "id": "79rkhkPUttrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Semi- supervised approach creates Opinion Target from stuff.\n",
        "stuff = ['software', 'application', 'service', 'power supply', 'sim card', 'display',\n",
        "         'storage space', 'sensor', 'wireless charging', 'design', 'cpu', 'accessories',\n",
        "         'camera','quality','time','condition','screen','price','case','build','access',\n",
        "         'battery','buy','power','switch','light','design','technology','radio','fashion'\n",
        "'product','charging','feature','touch','profile','car','slot','tables','construction',\n",
        "'period ','system','game','bottom','sound','blackberry charge','price anyone','price extra',\n",
        "'cord length','charge port',' phone','horizon charge','fraction price','charge ','key',\n",
        "'extension','internet','cheap','cover','speaker']\n",
        "\n"
      ],
      "metadata": {
        "id": "vI0TsNgStwvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Finding Similar waords of above Opinion Targets\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from itertools import chain\n",
        "\n",
        "stuff_OT = defaultdict(set)\n",
        "OT2 = set()\n",
        "\n",
        "synsets_set = defaultdict(set)\n",
        "hyponyms_set = defaultdict(set)\n",
        "for z in range(len(stuff)):\n",
        "    input_word = stuff[z]\n",
        "    OT1= set()\n",
        "    for i,j in enumerate(wn.synsets(input_word)):\n",
        "    #    print ('Meaning', i, 'NLTK ID: ', j.name())\n",
        "\n",
        "        hypernyms = ', '.join(list(chain(*[l.lemma_names() for l in j.hypernyms()])))\n",
        "    #    print ('Hypernyms:', hypernyms)\n",
        "        synsets_set[i].add(hypernyms)\n",
        "\n",
        "        hyponyms = ', '.join(list(chain(*[l.lemma_names() for l in j.hyponyms()])))\n",
        "    #    print ('Hyponyms:', hyponyms)\n",
        "        hyponyms_set[i].add(hyponyms)\n",
        "    #    print()\n",
        "\n",
        "        ho = [hypernyms]\n",
        "        for h in range(len(ho)):\n",
        "            temp_list = ho[h].split(', ')\n",
        "            if(temp_list != ['']):\n",
        "                for l in range(len(temp_list)):\n",
        "                    temp_word = ' '.join(temp_list[l].split('_'))\n",
        "                    OT2.add(temp_word)\n",
        "                    OT1.add(temp_word)\n",
        "\n",
        "        hy = [hypernyms]\n",
        "        for h in range(len(hy)):\n",
        "            temp_list = hy[h].split(', ')\n",
        "            if(temp_list != ['']):\n",
        "                for l in range(len(temp_list)):\n",
        "                    temp_word = ' '.join(temp_list[l].split('_'))\n",
        "                    OT2.add(temp_word)\n",
        "                    OT1.add(temp_word)\n",
        "    OT1 = list(OT1)\n",
        "    for i in range(len(OT1)):\n",
        "        stuff_OT[stuff[z]].add(OT1[i])"
      ],
      "metadata": {
        "id": "Wwik9gPetzyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyponyms_set_keys = hyponyms_set.keys()\n",
        "synsets_set_keys = synsets_set.keys()\n",
        "\n",
        "for z in range(len(stuff)):\n",
        "\n",
        "    for k in hyponyms_set_keys:\n",
        "        hy = hyponyms_set[k]\n",
        "        hy = list(hy)\n",
        "        for h in range(len(hy)):\n",
        "            temp_list = hy[h].split(', ')\n",
        "            if(temp_list != ['']):\n",
        "                for l in range(len(temp_list)):\n",
        "                    temp_word = ' '.join(temp_list[l].split('_'))\n",
        "                    OT2.add(temp_word)\n",
        "\n",
        "for z in range(len(stuff)):\n",
        "    for k in synsets_set_keys:\n",
        "        hy = synsets_set[k]\n",
        "        hy = list(hy)\n",
        "        for h in range(len(hy)):\n",
        "            temp_list = hy[h].split(', ')\n",
        "            if(temp_list != ['']):\n",
        "                for l in range(len(temp_list)):\n",
        "                    temp_word = ' '.join(temp_list[l].split('_'))\n",
        "                    OT2.add(temp_word)\n",
        "\n",
        "OT2 = list(OT2)\n",
        "\n",
        "list_of_subset = defaultdict(set)\n",
        "for i in range(len(stuff)):\n",
        "    stuff_OT[stuff[i]] = list(stuff_OT[stuff[i]])\n",
        "    for j in range(len(stuff_OT[stuff[i]])):\n",
        "        list_of_subset[i].add(stuff[i])\n",
        "        list_of_subset[i].add(stuff_OT[stuff[i]][j])\n",
        "\n",
        "for i in list_of_subset.keys():\n",
        "    list_of_subset[i] = list(list_of_subset[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "list_of_subset2 =[]\n",
        "for i in range(len(stuff)):\n",
        "    list_of_subset2.append(stuff[i])\n",
        "\n",
        "for i in list_of_subset.keys():\n",
        "    for j in range(len(list_of_subset[i])):\n",
        "        list_of_subset2.append(list_of_subset[i][j])\n",
        "\n",
        "stuff = list_of_subset2"
      ],
      "metadata": {
        "id": "KBDSgOpPt2kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Finding Opinion Words of above Opinion Target(Stuff + its similar words.)\n",
        "#         from pattern generated in step 3.\n",
        "\n",
        "OW = defaultdict(set)\n",
        "OT = defaultdict(set)\n",
        "OT_OW = defaultdict(set)\n",
        "OW_OT = defaultdict(set)\n",
        "\n",
        "# it has 1 OW\n",
        "pattern1_OT_OW =  defaultdict(set)\n",
        "pattern1_OW_OT =  defaultdict(set)\n",
        "p1_keys = pattern1.keys()\n",
        "for i in p1_keys:\n",
        "    if( pattern1[i] != []):\n",
        "        for j in range(len(pattern1[i])):\n",
        "            OT[i].add(pattern1[i][j][1][0])\n",
        "            OW[i].add(pattern1[i][j][0][0])\n",
        "            if(pattern1[i][j][1][0] in stuff):\n",
        "                OT_OW[pattern1[i][j][1][0]].add(pattern1[i][j][0][0])\n",
        "                OW_OT[pattern1[i][j][0][0]].add(pattern1[i][j][1][0])\n",
        "\n",
        "                pattern1_OT_OW[pattern1[i][j][1][0]].add(pattern1[i][j][0][0])\n",
        "                pattern1_OW_OT[pattern1[i][j][0][0]].add(pattern1[i][j][1][0])\n",
        "\n",
        "#it has 1 OW\n",
        "pattern2_OT_OW = defaultdict(set)\n",
        "pattern2_OW_OT = defaultdict(set)\n",
        "p2_keys = pattern2.keys()\n",
        "for i in p2_keys:\n",
        "    if( pattern2[i] != []):\n",
        "        for j in range(len(pattern2[i])):\n",
        "            target = pattern2[i][j][1][0] + \" \" + pattern2[i][j][2][0]\n",
        "            OT[i].add(target)\n",
        "            OW[i].add(pattern2[i][j][0][0])\n",
        "            if(pattern2[i][j][1][0] in stuff or pattern2[i][j][2][0] in stuff or target in stuff):\n",
        "                OT_OW[target].add(pattern2[i][j][0][0])\n",
        "                OW_OT[pattern2[i][j][0][0]].add(target)\n",
        "\n",
        "                pattern2_OT_OW[target].add(pattern2[i][j][0][0])\n",
        "                pattern2_OW_OT[pattern2[i][j][0][0]].add(target)\n",
        "\n",
        "\n",
        "# dont filter here\n",
        "# it has 2 OW pretty, good we use only pretty good combination\n",
        "pattern3_OW_OT = defaultdict(set)\n",
        "p3_keys = pattern3.keys()\n",
        "for i in p3_keys:\n",
        "    if( pattern3[i] != []):\n",
        "        for j in range(len(pattern3[i])):\n",
        "            target = pattern3[i][j][0][0] + \" \" + pattern3[i][j][1][0]\n",
        "            OW[i].add(target)\n",
        "\n",
        "            OW_OT[target].add('NO Opinion Target found')\n",
        "\n",
        "            pattern3_OW_OT[target].add('NO Opinion Target found')\n",
        "\n",
        "\n",
        "\n",
        "# we use near much, near, much word combinations in OW here\n",
        "pattern4_OT_OW = defaultdict(set)\n",
        "pattern4_OW_OT = defaultdict(set)\n",
        "p4_keys = pattern4.keys()\n",
        "for i in p4_keys:\n",
        "    if( pattern4[i] != []):\n",
        "        for j in range(len(pattern4[i])):\n",
        "            word = pattern4[i][j][0][0] + \" \" + pattern4[i][j][1][0]\n",
        "            OT[i].add(pattern4[i][j][2][0])\n",
        "            OW[i].add(word)\n",
        "            if(pattern4[i][j][2][0] in stuff):\n",
        "                OT_OW[pattern4[i][j][2][0]].add(word)\n",
        "\n",
        "                OW_OT[word].add(pattern4[i][j][2][0])\n",
        "\n",
        "                pattern4_OT_OW[pattern4[i][j][2][0]].add(word)\n",
        "\n",
        "                pattern4_OW_OT[word].add(pattern4[i][j][2][0])\n",
        "\n",
        "\n",
        "# dont filter here\n",
        "pattern5_OW_OT = defaultdict(set)\n",
        "p5_keys = pattern5.keys()\n",
        "for i in p5_keys:\n",
        "    if( pattern5[i] != []):\n",
        "        for j in range(len(pattern5[i])):\n",
        "            target = pattern5[i][j][0][0] + ' ' + pattern5[i][j][1][0]\n",
        "            OW[i].add(target)\n",
        "            OW_OT[target].add('No Opinion Target found')\n",
        "\n",
        "            pattern5_OW_OT[target].add('No Opinion Target found')\n",
        "\n",
        "\n",
        "# dont filter here\n",
        "pattern6_OW_OT = defaultdict(set)\n",
        "p6_keys = pattern6.keys()\n",
        "for i in p6_keys:\n",
        "    if( pattern6[i] != []):\n",
        "        for j in range(len(pattern6[i])):\n",
        "            target = pattern6[i][j][0][0] + \" \" + pattern6[i][j][2][0]\n",
        "            OW[i].add(target)\n",
        "            OW_OT[target].add('NO Opinion Target found')\n",
        "\n",
        "            pattern6_OW_OT[target].add('NO Opinion Target found')\n",
        "\n",
        "\n",
        "pattern7_OW_OT = defaultdict(set)\n",
        "pattern7_OT_OW = defaultdict(set)\n",
        "p7_keys = pattern7.keys()\n",
        "for i in p7_keys:\n",
        "    if( pattern7[i] != []):\n",
        "        for j in range(len(pattern7[i])):\n",
        "            target = pattern7[i][j][1][0]\n",
        "            OT[i].add(target)\n",
        "            OW[i].add(pattern7[i][j][0][0])\n",
        "\n",
        "            if(target in stuff):\n",
        "                OW_OT[pattern7[i][j][0][0]].add(target)\n",
        "                OT_OW[target].add(pattern7[i][j][0][0])\n",
        "\n",
        "                pattern7_OW_OT[pattern7[i][j][0][0]].add(target)\n",
        "                pattern7_OT_OW[target].add(pattern7[i][j][0][0])\n",
        "\n",
        "\n",
        "# dont filter here\n",
        "pattern_8_OW_OT = defaultdict(set)\n",
        "p8_keys = pattern8.keys()\n",
        "for i in p8_keys:\n",
        "    if( pattern8[i] != []):\n",
        "        for j in range(len(pattern8[i])):\n",
        "            target = pattern8[i][j][1][0]\n",
        "\n",
        "            OW[i].add(target)\n",
        "            OW_OT[target].add('No Opinion Target found')\n",
        "            pattern_8_OW_OT[target].add('No Opinion Target found')"
      ],
      "metadata": {
        "id": "OJTLSPnxt5XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Finding similar Words of above Opinion Words.\n",
        "\n",
        "OW_OT_key = OW_OT.keys()\n",
        "OW_list = list(OW_OT_key)\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from itertools import chain\n",
        "\n",
        "stuff_OW = defaultdict(set)\n",
        "OW2 = set()\n",
        "\n",
        "synsets_set = defaultdict(set)\n",
        "hyponyms_set = defaultdict(set)\n",
        "for z in range(len(OW_list)):\n",
        "    input_word = OW_list[z]\n",
        "    OW1= set()\n",
        "    for i,j in enumerate(wn.synsets(input_word)):\n",
        "    #    print ('Meaning', i, 'NLTK ID: ', j.name())\n",
        "\n",
        "        hypernyms = ', '.join(list(chain(*[l.lemma_names() for l in j.hypernyms()])))\n",
        "    #    print ('Hypernyms:', hypernyms)\n",
        "        synsets_set[i].add(hypernyms)\n",
        "\n",
        "        hyponyms = ', '.join(list(chain(*[l.lemma_names() for l in j.hyponyms()])))\n",
        "    #    print ('Hyponyms:', hyponyms)\n",
        "        hyponyms_set[i].add(hyponyms)\n",
        "    #    print()\n",
        "\n",
        "        ho = [hypernyms]\n",
        "        for h in range(len(ho)):\n",
        "            temp_list = ho[h].split(', ')\n",
        "            if(temp_list != ['']):\n",
        "                for l in range(len(temp_list)):\n",
        "                    temp_word = ' '.join(temp_list[l].split('_'))\n",
        "                    OW2.add(temp_word)\n",
        "                    OW1.add(temp_word)\n",
        "\n",
        "        hy = [hypernyms]\n",
        "        for h in range(len(hy)):\n",
        "            temp_list = hy[h].split(', ')\n",
        "            if(temp_list != ['']):\n",
        "                for l in range(len(temp_list)):\n",
        "                    temp_word = ' '.join(temp_list[l].split('_'))\n",
        "                    OW2.add(temp_word)\n",
        "                    OW1.add(temp_word)\n",
        "    OW1 = list(OW1)\n",
        "    for i in range(len(OW1)):\n",
        "        stuff_OW[OW_list[z]].add(OW1[i])\n",
        "\n",
        "\n",
        "hyponyms_set_keys = hyponyms_set.keys()\n",
        "synsets_set_keys = synsets_set.keys()\n",
        "\n",
        "for z in range(len(OW_list)):\n",
        "    for k in hyponyms_set_keys:\n",
        "        hy = hyponyms_set[k]\n",
        "        hy = list(hy)\n",
        "        for h in range(len(hy)):\n",
        "            temp_list = hy[h].split(', ')\n",
        "            if(temp_list != ['']):\n",
        "                for l in range(len(temp_list)):\n",
        "                    temp_word = ' '.join(temp_list[l].split('_'))\n",
        "                    OW2.add(temp_word)\n",
        "\n",
        "for z in range(len(OW_list)):\n",
        "    for k in synsets_set_keys:\n",
        "        hy = synsets_set[k]\n",
        "        hy = list(hy)\n",
        "        for h in range(len(hy)):\n",
        "            temp_list = hy[h].split(', ')\n",
        "            if(temp_list != ['']):\n",
        "                for l in range(len(temp_list)):\n",
        "                    temp_word = ' '.join(temp_list[l].split('_'))\n",
        "                    OW2.add(temp_word)\n",
        "\n",
        "OW_concept = []\n",
        "OW2 = list(OW2)\n",
        "for i in range(len(OW2)):\n",
        "    if(OW2[i] != ''):\n",
        "        OW_concept.append(OW2[i])\n",
        "\n",
        "for i in range(len(OW_list)):\n",
        "    OW_concept.append(OW_list[i])"
      ],
      "metadata": {
        "id": "lIlDv8Rct9qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Finding Opinion Weight of above similar Opinion Words ( from testimonial.sentiment.polarity )\n",
        "\n",
        "OW_OT = OW_concept\n",
        "OW_OT_key = OW_OT\n",
        "OT_OW_key = OT_OW.keys()\n",
        "\n",
        "OW_in_corpus = defaultdict(set)\n",
        "for i in corpus.keys():\n",
        "    for j in range(len(corpus[i])):\n",
        "        word = corpus[i][j].words\n",
        "        for k in range(len(word)):\n",
        "            if( word[k] in OW_OT_key):\n",
        "              OW_in_corpus[i].add(word[k])\n",
        "\n",
        "for i in OW_in_corpus.keys():\n",
        "    OW_in_corpus[i] = list(OW_in_corpus[i])\n",
        "\n",
        "testimonial_sentiment = defaultdict(list)\n",
        "testimonial_sentiment_polarity = defaultdict(list)\n",
        "\n",
        "for i in OW_in_corpus.keys():\n",
        "    for j in range(len(OW_in_corpus[i])):\n",
        "        testimonial = TextBlob(OW_in_corpus[i][j])\n",
        "        testimonial_sentiment[OW_in_corpus[i][j]].append(testimonial.sentiment)\n",
        "        testimonial_sentiment_polarity[OW_in_corpus[i][j]].append(testimonial.sentiment.polarity)\n",
        "\n",
        "OW_in_corpus_list = defaultdict(list)\n",
        "OW_in_corpus_value = defaultdict(list)\n",
        "for i in OW_in_corpus.keys():\n",
        "    for j in range(len(OW_in_corpus[i])):\n",
        "        word = [OW_in_corpus[i][j]]\n",
        "#        for k in range(len(word)):\n",
        "        if(word[0] in testimonial_sentiment_polarity.keys()):\n",
        "            polarity = testimonial_sentiment_polarity[word[0]]\n",
        "            OW_in_corpus_value[i].append(polarity[0])\n",
        "            OW_in_corpus_list[i].append(word[0])\n",
        "\n",
        "dictionary1 = dict()\n",
        "dictionary2 = dict()\n",
        "key_value_pair = defaultdict(list)\n",
        "for i in range(len(OW_in_corpus_list)):\n",
        "    for j in range(len(OW_in_corpus_list[i])):\n",
        "#        dictionary1[OW_in_corpus_value[i][j]] = OW_in_corpus_list[i][j]\n",
        "        dictionary2[ OW_in_corpus_list[i][j]] = OW_in_corpus_value[i][j]"
      ],
      "metadata": {
        "id": "71JIy5V2uC9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Creating Words and its score in tuple_word_score\n",
        "\n",
        "score = defaultdict(list)\n",
        "words = OW_in_corpus_list\n",
        "OW_in_corpus_value_key = OW_in_corpus_value.keys()\n",
        "for i in OW_in_corpus_value_key:\n",
        "    for j in range(len(OW_in_corpus_value[i])):\n",
        "        score[i].append(OW_in_corpus_value[i][j])\n",
        "\n",
        "tuple_word_score = defaultdict(list)\n",
        "for i in OW_in_corpus_value_key:\n",
        "    for j in range(len(OW_in_corpus_value[i])):\n",
        "        tuple_word_score[i].append((words[i][j], score[i][j]))\n",
        "\n",
        "Opinion_Words = tuple_word_score\n",
        "\n",
        "list_Opinion_Words = []\n",
        "for i in range(len(Opinion_Words)):\n",
        "    list_Opinion_Words.append(Opinion_Words[i])"
      ],
      "metadata": {
        "id": "aKmhRnqeuGoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Finding Maximum scored words.\n",
        "\n",
        "max_abs_score = defaultdict(list)\n",
        "for i in range(len(tuple_word_score)):\n",
        "    maxx = 0\n",
        "    for j in range(len(tuple_word_score[i])):\n",
        "        temp = abs(tuple_word_score[i][j][1])\n",
        "        if(temp > maxx):\n",
        "            maxx = abs(tuple_word_score[i][j][1])\n",
        "            maxx_word = tuple_word_score[i][j][0]\n",
        "    max_abs_score[i].append((maxx_word, maxx))"
      ],
      "metadata": {
        "id": "WoW8LtMiuJLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxx_OW_value = defaultdict(set)\n",
        "for i in range(len(max_abs_score)):\n",
        "    maxx_OW_value[max_abs_score[i][0][0]].add(max_abs_score[i][0][1])\n",
        "\n",
        "New_OW = maxx_OW_value.keys()"
      ],
      "metadata": {
        "id": "hcajEx3EuL1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step11: Collecting important opinion words.\n",
        "\n",
        "list_max_abs_score = []\n",
        "for i in range(len(max_abs_score)):\n",
        "    list_max_abs_score.append(max_abs_score[i])"
      ],
      "metadata": {
        "id": "eUCQqyIIuOLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step12: Searching it again\n",
        "\n",
        "OW_in_corpus2 = defaultdict(set)\n",
        "for i in range(len(corpus)):\n",
        "    for j in range(len(corpus[i])):\n",
        "        word = corpus[i][j].words\n",
        "        for k in range(len(word)):\n",
        "            if( word[k] in New_OW):\n",
        "              OW_in_corpus2[i].add(word[k])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in OW_in_corpus2.keys():\n",
        "    OW_in_corpus2[i] = list(OW_in_corpus2[i])\n",
        "\n",
        "\n",
        "\n",
        "testimonial_sentiment = defaultdict(list)\n",
        "testimonial_sentiment_polarity = defaultdict(list)\n",
        "\n",
        "for i in OW_in_corpus2.keys():\n",
        "    for j in range(len(OW_in_corpus2[i])):\n",
        "        testimonial = TextBlob(OW_in_corpus2[i][j])\n",
        "        testimonial_sentiment[OW_in_corpus2[i][j]].append(testimonial.sentiment)\n",
        "        testimonial_sentiment_polarity[OW_in_corpus2[i][j]].append(testimonial.sentiment.polarity)\n",
        "\n",
        "OW_in_corpus_list = defaultdict(list)\n",
        "OW_in_corpus_value = defaultdict(list)\n",
        "for i in OW_in_corpus2.keys():\n",
        "    for j in range(len(OW_in_corpus2[i])):\n",
        "        word = [OW_in_corpus2[i][j]]\n",
        "#        for k in range(len(word)):\n",
        "        if(word[0] in testimonial_sentiment_polarity.keys()):\n",
        "            polarity = testimonial_sentiment_polarity[word[0]]\n",
        "            OW_in_corpus_value[i].append(polarity[0])\n",
        "            OW_in_corpus_list[i].append(word[0])\n",
        "\n",
        "\n",
        "dictionary1 = dict()\n",
        "dictionary2 = dict()\n",
        "key_value_pair = defaultdict(list)\n",
        "for i in range(len(OW_in_corpus_list)):\n",
        "    for j in range(len(OW_in_corpus_list[i])):\n",
        "#        dictionary1[OW_in_corpus_value[i][j]] = OW_in_corpus_list[i][j]\n",
        "        dictionary2[ OW_in_corpus_list[i][j]] = OW_in_corpus_value[i][j]\n",
        "\n"
      ],
      "metadata": {
        "id": "nhMSirmQuR5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "score = defaultdict(list)\n",
        "words = OW_in_corpus_list\n",
        "OW_in_corpus_value_key = OW_in_corpus_value.keys()\n",
        "for i in OW_in_corpus_value_key:\n",
        "    for j in range(len(OW_in_corpus_value[i])):\n",
        "        score[i].append(OW_in_corpus_value[i][j])\n",
        "\n",
        "\n",
        "tuple_word_score = defaultdict(list)\n",
        "for i in OW_in_corpus_value_key:\n",
        "    for j in range(len(OW_in_corpus_value[i])):\n",
        "        tuple_word_score[i].append((words[i][j], score[i][j]))"
      ],
      "metadata": {
        "id": "2k-yubCXuVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_score = defaultdict(list)\n",
        "for i in range(len(tuple_word_score)):\n",
        "    final_score[i].append(0)\n",
        "\n",
        "for i in range(len(tuple_word_score)):\n",
        "    for j in range(len(tuple_word_score[i])):\n",
        "        final_score[i].append(tuple_word_score[i][j][1])\n",
        "\n",
        "final_score = defaultdict(list)\n",
        "for i in range(len(tuple_word_score)):\n",
        "    final_score[i].append(0)\n",
        "\n",
        "for i in range(len(tuple_word_score)):\n",
        "    for j in range(len(tuple_word_score[i])):\n",
        "        final_score[i].append(tuple_word_score[i][j][1])"
      ],
      "metadata": {
        "id": "iJkvEX2duX43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 13: Creating Average\n",
        "average_score = defaultdict(list)\n",
        "for i in range(len(final_score)):\n",
        "    average_score[i].append(np.mean(final_score[i]))\n",
        "\n",
        "\n",
        "for i in range(len(average_score)):\n",
        "    if(np.isnan(average_score[i]) == True):\n",
        "        average_score[i] = [0]\n",
        "\n",
        "list_average_score = []\n",
        "for i in range(len(average_score)):\n",
        "    list_average_score.append(average_score[i])"
      ],
      "metadata": {
        "id": "qcC9FxrcuafQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = []\n",
        "for i in average_score.keys():\n",
        "    temp = set()\n",
        "    for j in range(len(OW_in_corpus[i])):\n",
        "        temp.add(OW_in_corpus[i][j])\n",
        "    X_train.append(temp)\n",
        "\n",
        "l = set()\n",
        "for i in range(len(X_train)):\n",
        "    X_train[i] = list (X_train[i])\n",
        "    for j in range(len(X_train[i])):\n",
        "        l.add(X_train[i][j])\n",
        "\n",
        "l =list(l)\n",
        "\n",
        "import pandas as pd\n",
        "da = pd.DataFrame(columns = l, index = OW_in_corpus.keys(), data = 0)\n",
        "\n",
        "k = list(OW_in_corpus.keys())\n",
        "\n",
        "for m in range(len(k)):\n",
        "    for j in range(len(l)):\n",
        "        if(l[j] in OW_in_corpus[k[m]]):\n",
        "            da.iloc[m][l[j]] = 1\n",
        "\n",
        "X_train = da.iloc[:, 0:]\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "y_train = []\n",
        "for i in average_score.keys():\n",
        "    y_train.append(average_score[i][0])\n",
        "\n",
        "r = (max(y_train) - min(y_train)) / 4\n",
        "m = min(y_train)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "y = []\n",
        "for i in range(len(y_train)):\n",
        "    y.append(y_train[i])\n",
        "\n",
        "X = X_train\n",
        "\n",
        "for i in range(len(y)):\n",
        "    if(y[i] <= m):\n",
        "        y[i] = 'Negative'\n",
        "    elif(y[i] <= m+r and y[i] > m):\n",
        "        y[i] = 'Negative'\n",
        "    elif(y[i] <= m+r+r and y[i] > m+r):\n",
        "        y[i] = 'Negative'\n",
        "    elif(y[i] <= m+r+r+r and y[i] > m+r+r):\n",
        "        y[i] = 'Positive'\n",
        "    elif(y[i] <= m+r+r+r+r and y[i] > m+r+r+r):\n",
        "        y[i] = 'Positive'\n",
        "\n",
        "y_set = defaultdict(list)\n",
        "\n",
        "\n",
        "key = average_score.keys()\n",
        "key = list(key)\n",
        "\n",
        "for i in range(len(key)):\n",
        "    y_set[key[i]].append(y[i])\n",
        "\n",
        "\n",
        "da['calculated rating'] = [y_set[i] for i in y_set.keys()]\n",
        "\n",
        "\n",
        "rev_rate = []\n",
        "rate = dataset['rating']\n",
        "for i in y_set.keys():\n",
        "    rev_rate.append(rate[i])\n",
        "\n",
        "for i in range(len(rev_rate)):\n",
        "    if(rev_rate[i] == 1.0):\n",
        "        rev_rate[i] = 'Negative'\n",
        "    elif(rev_rate[i] == 2.0):\n",
        "        rev_rate[i] = 'Negative'\n",
        "    elif(rev_rate[i] == 3.0):\n",
        "        rev_rate[i] = 'Negative'\n",
        "    elif(rev_rate[i] == 4.0):\n",
        "        rev_rate[i] = 'Positive'\n",
        "    elif(rev_rate[i] == 5.0):\n",
        "        rev_rate[i] = 'Positive'\n",
        "\n",
        "\n",
        "list_tuple_word_score = []\n",
        "for i in range(len(tuple_word_score)):\n",
        "    list_tuple_word_score.append(tuple_word_score[i])\n",
        "\n",
        "comparision_dataframe = pd.DataFrame()\n",
        "\n",
        "comparision_dataframe['Original Review'] = dataset['reviewText']\n",
        "comparision_dataframe['Opinion Words'] = list_Opinion_Words\n",
        "comparision_dataframe['Maximum scored Opinion Words'] = list_max_abs_score\n",
        "comparision_dataframe['Final Opnion Words'] = list_tuple_word_score\n",
        "comparision_dataframe['Average'] = list_average_score\n",
        "comparision_dataframe['Reviewer Rating'] = rev_rate\n",
        "comparision_dataframe['Calculated rating'] = y\n",
        "\n",
        "\n",
        "save = comparision_dataframe.to_csv(sep=',')\n",
        "text_file = open(\"train_file.csv\", \"w\")\n",
        "text_file.write(save)\n",
        "text_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9p0An5uuc6x",
        "outputId": "4f185d49-b979-47df-ad3c-26b04c5a982b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-a5b9ba013a64>:26: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
            "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
            "A typical example is when you are setting values in a column of a DataFrame, like:\n",
            "\n",
            "df[\"col\"][row_indexer] = value\n",
            "\n",
            "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "  da.iloc[m][l[j]] = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14:  Making the Confusion Matrix\n",
        "\n",
        "y = comparision_dataframe['Calculated rating']\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#cm = confusion_matrix(y_test, y_pred)\n",
        "cm = confusion_matrix(rev_rate, y)\n",
        "\n",
        "accuracy = 0\n",
        "for i in range(len(cm)):\n",
        "    for j in range(len(cm)):\n",
        "        if(i == j):\n",
        "            accuracy = accuracy + cm[i][j]\n",
        "accuracy = accuracy/ len(rev_rate) * 100\n",
        "\n",
        "\n",
        "TP = np.diag(cm)\n",
        "FP = np.sum(cm, axis=0) - TP\n",
        "FN = np.sum(cm, axis=1) - TP\n",
        "\n",
        "num_classes = len(cm)\n",
        "TN = []\n",
        "for i in range(num_classes):\n",
        "    temp = np.delete(cm, i, 0)    # delete ith row\n",
        "    temp = np.delete(temp, i, 1)  # delete ith column\n",
        "    TN.append(sum(sum(temp)))\n",
        "\n",
        "l = len(rev_rate)\n",
        "for i in range(num_classes):\n",
        "    print(TP[i] + FP[i] + FN[i] + TN[i] == l)\n",
        "\n",
        "\n",
        "precision = TP/(TP+FP)\n",
        "recall = TP/(TP+FN)\n",
        "FScore = 2*(recall * precision) / (recall + precision)\n",
        "\n",
        "DataFrame = pd.DataFrame()\n",
        "DataFrame['precision'] = precision\n",
        "DataFrame['recall'] = recall\n",
        "DataFrame['FScore'] = FScore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaX6M8ZqugV6",
        "outputId": "ff310892-f1bc-431d-b98c-5d84625370f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "QepfLsFdxHuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 15: Testing\n",
        "\n",
        "New_sentence = \"this phone is gifted to me by my friend.oh my god!!. i am not much impreseed by the sound quality. It isn't the one i expected. i am disappointed by its short term life.\"\n",
        "New_rating = 'Negative'\n",
        "New_sentence = ' This is a wonderful screen touch!! can it get better than this? i am so much excited!!'\n",
        "New_rating = 'Positive'\n",
        "\n"
      ],
      "metadata": {
        "id": "OSoMVbDsuiv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1: pre processing\n",
        "\n",
        "# isn't = is not\n",
        "New_sentence = decontracted(New_sentence)\n",
        "\n",
        "# removed Unwanted symbols, lower case\n",
        "comment_dict2 = defaultdict(list)\n",
        "for i in range(1):\n",
        "    sentence = re.sub('[^a-zA-Z.]',' ', New_sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.split('.')\n",
        "    for k in range(len(sentence)):\n",
        "        review = sentence[k].split()\n",
        "        review = [word for word in review if not word in set(stopwords.words('english'))]\n",
        "        sentence[k] =  ' '.join(review)\n",
        "        comment_dict2[i].append(sentence[k])\n",
        "\n",
        "#delete unwanted '' words\n",
        "for j in range(len(comment_dict2)):\n",
        "    comment_dict2[j] = [comment_dict2[j][i] for i in range(len(comment_dict2[j])) if comment_dict2[j][i] not in '']\n",
        "\n",
        "Text = defaultdict(list)\n",
        "for i in range(len(comment_dict2)):\n",
        "    Text[i].append(('. '.join(comment_dict2[i][j] for j in range(len(comment_dict2[i])))))\n",
        "\n",
        "# spelling correction\n",
        "for i in range(len(Text)):\n",
        "    b = TextBlob(Text[i][0])\n",
        "    Text[i] = b.correct()\n",
        "\n",
        "# creating corpus\n",
        "corpus2 = defaultdict(set)\n",
        "for i in range(len(Text)):\n",
        "    wiki = Text[i]\n",
        "    corpus2[i] = wiki.sentences\n",
        "\n",
        "corpus_key2 = corpus2.keys()\n",
        "corpus_list2 = defaultdict(list)\n",
        "\n",
        "for i in corpus_key2:\n",
        "    for j in range(len(corpus2[i])):\n",
        "        word = ' '.join(corpus2[i][j].words)\n",
        "        corpus_list2[i].append(word)\n",
        "\n",
        "Text = \"\"\n",
        "for i in range(len(corpus_list2)):\n",
        "    temp = corpus_list2[i]\n",
        "    for j in temp:\n",
        "        Text += str(j) + ' '"
      ],
      "metadata": {
        "id": "dYxSjGhbumGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step2 of testing : Use OW and find its score\n",
        "\n",
        "list_max_abs_score = []\n",
        "for i in range(len(max_abs_score)):\n",
        "    list_max_abs_score.append(max_abs_score[i])\n",
        "\n",
        "t = TextBlob(Text)\n",
        "OW_in_corpus22 = defaultdict(set)\n",
        "for i in range(len(corpus2)):\n",
        "    for j in range(len(corpus2[i])):\n",
        "        word = t.words\n",
        "        for k in range(len(word)):\n",
        "            if( word[k] in New_OW):\n",
        "              OW_in_corpus22[i].add(word[k])\n",
        "\n",
        "for i in OW_in_corpus22.keys():\n",
        "    OW_in_corpus22[i] = list(OW_in_corpus22[i])\n",
        "\n",
        "testimonial_sentiment = defaultdict(list)\n",
        "testimonial_sentiment_polarity = defaultdict(list)\n",
        "OW_in_corpus2 = OW_in_corpus22\n",
        "\n",
        "for i in OW_in_corpus2.keys():\n",
        "    for j in range(len(OW_in_corpus2[i])):\n",
        "        testimonial = TextBlob(OW_in_corpus2[i][j])\n",
        "        testimonial_sentiment[OW_in_corpus2[i][j]].append(testimonial.sentiment)\n",
        "        testimonial_sentiment_polarity[OW_in_corpus2[i][j]].append(testimonial.sentiment.polarity)\n",
        "\n",
        "OW_in_corpus_list = defaultdict(list)\n",
        "OW_in_corpus_value = defaultdict(list)\n",
        "for i in OW_in_corpus2.keys():\n",
        "    for j in range(len(OW_in_corpus2[i])):\n",
        "        word = [OW_in_corpus2[i][j]]\n",
        "#        for k in range(len(word)):\n",
        "        if(word[0] in testimonial_sentiment_polarity.keys()):\n",
        "            polarity = testimonial_sentiment_polarity[word[0]]\n",
        "            OW_in_corpus_value[i].append(polarity[0])\n",
        "            OW_in_corpus_list[i].append(word[0])\n",
        "\n",
        "dictionary1 = dict()\n",
        "dictionary2 = dict()\n",
        "key_value_pair = defaultdict(list)\n",
        "for i in range(len(OW_in_corpus_list)):\n",
        "    for j in range(len(OW_in_corpus_list[i])):\n",
        "#        dictionary1[OW_in_corpus_value[i][j]] = OW_in_corpus_list[i][j]\n",
        "        dictionary2[ OW_in_corpus_list[i][j]] = OW_in_corpus_value[i][j]\n",
        "\n",
        "import numpy as np\n",
        "score = defaultdict(list)\n",
        "words = OW_in_corpus_list\n",
        "OW_in_corpus_value_key = OW_in_corpus_value.keys()\n",
        "for i in OW_in_corpus_value_key:\n",
        "    for j in range(len(OW_in_corpus_value[i])):\n",
        "        score[i].append(OW_in_corpus_value[i][j])\n",
        "\n",
        "tuple_word_score = defaultdict(list)\n",
        "for i in OW_in_corpus_value_key:\n",
        "    for j in range(len(OW_in_corpus_value[i])):\n",
        "        tuple_word_score[i].append((words[i][j], score[i][j]))\n",
        "\n",
        "final_score = defaultdict(list)\n",
        "for i in range(len(tuple_word_score)):\n",
        "    final_score[i].append(0)\n",
        "\n",
        "for i in range(len(tuple_word_score)):\n",
        "    for j in range(len(tuple_word_score[i])):\n",
        "        final_score[i].append(tuple_word_score[i][j][1])\n",
        "\n",
        "final_score = defaultdict(list)\n",
        "for i in range(len(tuple_word_score)):\n",
        "    final_score[i].append(0)\n",
        "\n",
        "for i in range(len(tuple_word_score)):\n",
        "    for j in range(len(tuple_word_score[i])):\n",
        "        final_score[i].append(tuple_word_score[i][j][1])\n",
        "\n",
        "average_score = defaultdict(list)\n",
        "for i in range(len(final_score)):\n",
        "    average_score[i].append(np.mean(final_score[i]))\n",
        "\n",
        "\n",
        "for i in range(len(average_score)):\n",
        "    if(np.isnan(average_score[i]) == True):\n",
        "        average_score[i] = [0]\n",
        "\n",
        "list_avg = []\n",
        "for i in range(len(average_score)):\n",
        "    list_avg.append(average_score[i][0])\n",
        "\n",
        "list_tuple_word_score = []\n",
        "for i in range(len(tuple_word_score)):\n",
        "    list_tuple_word_score.append(tuple_word_score[i])"
      ],
      "metadata": {
        "id": "AvNF59WMureo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = []\n",
        "for i in average_score.keys():\n",
        "    y_train.append(average_score[i][0])\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "y = []\n",
        "for i in range(len(y_train)):\n",
        "    y.append(y_train[i])\n",
        "\n",
        "X = X_train\n",
        "\n",
        "for i in range(len(y)):\n",
        "    if(y[i] <= m):\n",
        "        y[i] = 'Negative'\n",
        "    elif(y[i] <= m+r and y[i] > m):\n",
        "        y[i] = 'Negative'\n",
        "    elif(y[i] <= m+r+r and y[i] > m+r):\n",
        "        y[i] = 'Negative'\n",
        "    elif(y[i] <= m+r+r+r and y[i] > m+r+r):\n",
        "        y[i] = 'Positive'\n",
        "    elif(y[i] <= m+r+r+r+r and y[i] > m+r+r+r):\n",
        "        y[i] = 'Positive'\n",
        "\n",
        "c_dataframe = pd.DataFrame()\n",
        "c_dataframe['Original Testing Review'] = [New_sentence]\n",
        "c_dataframe['Processed Testing Review'] = [Text]\n",
        "c_dataframe['Opinion Words Obtained'] = list_tuple_word_score\n",
        "c_dataframe['Average of Opinion Words'] = average_score[0]\n",
        "c_dataframe['Original Rating'] = New_rating\n",
        "c_dataframe['Predicted Rating'] = y"
      ],
      "metadata": {
        "id": "OgLG3ZHkut9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_train\n",
        "y = comparision_dataframe['Calculated rating']\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "gc8gxg8ZuveG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Fitting Logistic Regression to the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(random_state = 0)\n",
        "\n",
        "# Fitting DecisionTreeClassifier to the Training set\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier( criterion = 'entropy', random_state = 0)\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "#from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#from sklearn.naive_bayes import BernoulliNB\n",
        "#classifier = GaussianNB()\n",
        "classifier = MultinomialNB() # do no standartize\n",
        "#classifier = BernoulliNB()\n",
        "\n",
        "# Fitting Random Forest Classification to the Training set\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier( n_estimators = 10, criterion = 'entropy', random_state = 0, n_jobs = -1)\n",
        "\n",
        "# Fitting  K-Nearest Neighbors K-NN to the Training set\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "\n",
        "# Fitting Linear Support Vector Machine Linear SVM to the Training set\n",
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel= 'sigmoid', random_state = 0)\n",
        "\n",
        "# Fitting Linear Support Vector Machine Linear SVM to the Training set\n",
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel= 'rbf', random_state = 0)\n",
        "\n",
        "\n",
        "# Fitting Linear Support Vector Machine Linear SVM to the Training set\n",
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel= 'poly', random_state = 0)\n",
        "\n",
        "# Fitting Linear Support Vector Machine Linear SVM to the Training set\n",
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel= 'linear', random_state = 0)"
      ],
      "metadata": {
        "id": "yAzD1vIdux4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train, y_train)\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "accuracy = 0\n",
        "for i in range(len(cm)):\n",
        "    for j in range(len(cm)):\n",
        "        if(i == j):\n",
        "            accuracy = accuracy + cm[i][j]\n",
        "accuracy = accuracy/ len(y_test) * 100\n",
        "\n",
        "\n",
        "# 10 fold cross validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv=10, n_jobs = -1)\n",
        "\n",
        "accuracy_by_10fold_cv = (sum(accuracies) / 10)\n",
        "accuracy_by_10fold_cv = accuracies.mean() * 100\n",
        "std = accuracies.std() * 100\n",
        "accuracy = accuracy_by_10fold_cv\n",
        "\n",
        "TP = np.diag(cm)\n",
        "FP = np.sum(cm, axis=0) - TP\n",
        "FN = np.sum(cm, axis=1) - TP\n",
        "\n",
        "num_classes = len(cm)\n",
        "TN = []\n",
        "for i in range(num_classes):\n",
        "    temp = np.delete(cm, i, 0)    # delete ith row\n",
        "    temp = np.delete(temp, i, 1)  # delete ith column\n",
        "    TN.append(sum(sum(temp)))\n",
        "l = len(y_test)\n",
        "for i in range(num_classes):\n",
        "    print(TP[i] + FP[i] + FN[i] + TN[i] == l)\n",
        "\n",
        "precision = TP/(TP+FP)\n",
        "recall = TP/(TP+FN)\n",
        "FScore = 2*(recall * precision) / (recall + precision)\n",
        "\n",
        "DataFrame = pd.DataFrame()\n",
        "DataFrame['precision'] = precision\n",
        "DataFrame['recall'] = recall\n",
        "DataFrame['FScore'] = FScore\n",
        "\n",
        "print('\\nDataFrame:\\n', DataFrame)\n",
        "print('\\n\\nConfusion Matrix:\\n', cm)\n",
        "print('\\n\\nAccuracy: ', accuracy)\n",
        "\n",
        "\n",
        "\n",
        "print('\\n\\nOpinionWords:\\n', OW_in_corpus)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr_MdNoKu1pm",
        "outputId": "b1ef8a40-991c-4ca7-a80e-9a482edc7223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "\n",
            "DataFrame:\n",
            "    precision  recall    FScore\n",
            "0   0.681818    0.60  0.638298\n",
            "1   0.811321    0.86  0.834951\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[15 10]\n",
            " [ 7 43]]\n",
            "\n",
            "\n",
            "Accuracy:  78.22134387351778\n",
            "\n",
            "\n",
            "OpinionWords:\n",
            " defaultdict(<class 'set'>, {0: ['product', 'shape', 'popping', 'like', 'buy', 'look', 'rounded', 'good', 'stick'], 1: ['like', 'review', 'stay', 'great', 'share', 'stylish', 'work', 'stick'], 2: ['used', 'quality', 'look', 'almost', 'make', 'great', 'stylish', 'believe', 'awesome'], 3: ['nice', 'condition', 'free', 'perfect', 'claim', 'get', 'screen', 'great', 'big', 'received', 'deal', 'ordered'], 4: ['used', 'multiple', 'elevated', 'great', 'awesome'], 5: ['price', 'easy', 'like', 'purchase', 'button', 'make', 'well', 'worth'], 6: ['ask', 'came', 'got', 'cut', 'come', 'great', 'people'], 7: ['first', 'waste', 'charge'], 8: ['build', 'leave', 'start', 'return', 'end', 'access', 'full', 'solid', 'good'], 9: ['double', 'buy', 'easy', 'access', 'promised', 'fantastic', 'great', 'stylish'], 10: ['complain', 'perfectly'], 11: ['weight', 'switch', 'handy', 'indicate', 'price', 'add', 'came', 'galaxy', 'first', 'much', 'well', 'charge', 'level', 'slim', 'worth'], 12: ['insert', 'perform', 'easy', 'boost', 'experience', 'love'], 13: ['strong', 'take', 'whole', 'light', 'charge', 'design', 'happen', 'made', 'completely', 'center', 'resistant', 'buy', 'like', 'single', 'night', 'manner', 'positive', 'cost', 'let', 'tell', 'sleek', 'wear', 'hear', 'ultra', 'became', 'rechargeable', 'going', 'people', 'build', 'leave', 'purchase', 'brand', 'overbear', 'dropped', 'best', 'shock'], 14: ['got', 'awesome', 'new', 'needed'], 15: ['near', 'purchase', 'overbear', 'great', 'ordered'], 16: ['called', 'like', 'bear', 'get', 'great', 'best', 'got'], 17: ['expensive', 'reliable', 'well', 'excellent', 'veryalue'], 18: ['stock', 'charge', 'fine', 'work', 'last', 'galaxy', 'long', 'high', 'expensive', 'used', 'output', 'seem', 'bought', 'tested', 'much', 'provide', 'dual', 'unit', 'note', 'assume', 'cheap'], 19: ['completely', 'first', 'small', 'great', 'couple', 'waste'], 20: ['convenient', 'love', 'many', 'sleek'], 21: ['need', 'charge', 'find'], 22: ['compact', 'like', 'small', 'charge', 'need'], 23: ['tell', 'nice', 'led', 'able', 'charge', 'sure'], 24: ['keep', 'well', 'charge', 'way', 'need'], 25: ['tried', 'little', 'bought', 'review', 'great'], 26: ['adapted', 'get', 'disappointed', 'pad', 'work', 'cheap'], 27: ['tell', 'nice', 'near', 'like', 'stop', 'listen', 'much', 'charge', 'made', 'gave', 'fine', 'consider'], 28: ['side'], 29: ['ask', 'fashion', 'buy', 'get', 'order', 'great', 'timely'], 30: ['emit', 'new', 'product', 'abuse', 'perform', 'like', 'glow', 'love', 'awesome'], 31: ['different', 'give', 'like', 'jack', 'stay', 'much', 'great'], 32: ['connect', 'quality', 'stop', 'purchased', 'bad', 'side', 'cheap'], 33: ['like', 'finish', 'great', 'charge', 'wait', 'dont'], 34: ['price', 'came', 'believe', 'worth', 'come', 'good', 'last'], 35: ['plastic', 'last', 'long', 'cap', 'great', 'expected', 'piece', 'cheap'], 36: ['product', 'condition', 'easy', 'quality', 'shown', 'much', 'great', 'good', 'excellent'], 37: ['use', 'think', 'galaxy', 'purchased', 'charge', 'note', 'need', 'put', 'pad'], 38: ['great', 'charge'], 39: ['light', 'con', 'overcome', 'fine', 'loved', 'work', 'came', 'easily', 'purchased', 'many', 'clear', 'product', 'cool', 'feature', 'top', 'total', 'plug', 'pro', 'piece', 'put'], 40: ['tax', 'recognize', 'use', 'bought', 'charge', 'high'], 41: ['compact', 'bought', 'small', 'great', 'charge', 'needed', 'cheap'], 42: ['nice', 'led', 'low', 'samson', 'touch', 'light', 'great', 'charge'], 43: ['well', 'universal', 'great', 'charge'], 44: ['forced', 'give', 'look', 'solid', 'light', 'quit', 'bad', 'charge', 'design', 'fine', 'device', 'led', 'remove', 'tight', 'guess', 'well', 'cost', 'reason', 'high', 'tell', 'tightly', 'price', 'seem', 'end', 'yet', 'range', 'short', 'fit', 'really', 'charged', 'top', 'handy', 'better', 'notice', 'found', 'back', 'pro', 'made', 'sure', 'period', 'go', 'cheap'], 45: ['easy', 'great', 'gas', 'charge', 'short', 'veryalue'], 46: ['tried', 'fire', 'product', 'destroy', 'test', 'bought', 'first', 'hazard', 'felt', 'electrical', 'pulled'], 47: ['people', 'extra', 'spare', 'charge', 'pair', 'good', 'go'], 48: ['way', 'better', 'gas', 'charge', 'love'], 49: ['bought', 'charge'], 50: ['product', 'price', 'deal', 'easy', 'like', 'find', 'expected', 'received', 'good', 'etc'], 51: ['expect', 'light', 'charge', 'travel', 'fine', 'work', 'last', 'device', 'led', 'great', 'happy', 'say', 'pad', 'use', 'product', 'price', 'plugged', 'hope', 'deliver', 'much', 'electrical', 'really', 'correctly', 'leave', 'little', 'hand', 'pleased', 'couple', 'made', 'daily', 'needed'], 52: ['product', 'price', 'little', 'free', 'low', 'purchased', 'charge', 'love', 'good'], 53: ['use', 'overall', 'deal', 'light', 'night', 'well', 'charge', 'good', 'bright', 'needed'], 54: ['love', 'touch', 'happy', 'way'], 55: ['cool', 'paint', 'effect', 'design', 'cost'], 56: ['pink', 'cover', 'rubberized', 'product', 'green', 'little', 'came', 'picture', 'like', 'worried', 'armor', 'order', 'expected', 'design'], 57: ['change', 'love', 'really', 'many'], 58: ['white', 'arrive', 'long', 'fit', 'perfectly'], 59: ['price', 'mark', 'believe', 'order', 'worth'], 60: ['perfectly', 'use', 'fit', 'great'], 61: ['quickly', 'cover', 'soft', 'rubberized', 'buy', 'received', 'look', 'expected', 'many', 'design', 'good', 'excellent'], 62: ['cover', 'little', 'old', 'like', 'look', 'order', 'feel', 'new'], 63: ['deal', 'purchase', 'shortly', 'first', 'back', 'guess', 'sometimes', 'received', 'started', 'feel', 'good', 'loved', 'really'], 64: ['protection', 'dark', 'easy', 'lot', 'get', 'color', 'put', 'person', 'best'], 65: ['liked', 'help', 'change', 'look', 'great', 'way', 'really'], 66: ['little', 'give', 'like', 'worth', 'side', 'dropped', 'many', 'good', 'break'], 67: ['protection', 'box', 'utter', 'want', 'easy', 'bought', 'get', 'extra', 'expected', 'great', 'show', 'protected', 'snap', 'fine', 'go'], 68: ['lot', 'color', 'love', 'favorite', 'really'], 69: ['used', 'like', 'get', 'paint', 'broken', 'got'], 70: ['pink', 'original', 'favorite', 'color', 'much', 'reason', 'loved'], 71: ['used', 'rubberized', 'switch', 'picture', 'like', 'awhile', 'look', 'cut', 'love', 'really'], 72: ['pink', 'nice', 'tried', 'trouble', 'price', 'give', 'want', 'remove', 'received', 'feel', 'yet', 'change', 'cheap', 'black'], 73: ['bought', 'many', 'received', 'stylish', 'best'], 74: ['switch', 'easy', 'like', 'apply', 'many', 'protected', 'love'], 75: ['liked', 'use', 'see'], 76: ['good', 'fine', 'charge'], 77: ['ask', 'genuine', 'product', 'condition', 'price', 'think', 'brand', 'promised', 'much', 'charge', 'new', 'perfectly'], 78: ['incise', 'use', 'product', 'price', 'forget', 'keep', 'galaxy', 'bought', 'great', 'charge', 'travel', 'work'], 79: ['fast', 'end', 'take', 'leaving', 'meet', 'well', 'need', 'high', 'loved', 'go', 'model'], 80: ['adapted', 'different', 'make', 'well', 'charge', 'come', 'broken', 'perfectly'], 81: ['just', 'fast', 'charge', 'need', 'direct', 'wall'], 82: ['bit', 'right', 'use', 'seem', 'galaxy', 'great', 'charge', 'side', 'short', 'good', 'fit', 'last'], 83: ['afford', 'price', 'buy', 'unfordable', 'charge', 'good'], 84: ['use', 'price', 'low', 'galaxy', 'extra', 'well', 'charge', 'great', 'note', 'travel'], 85: ['stay', 'charge', 'need', 'buy', 'like', 'issue', 'connected', 'worry', 'happy', 'great', 'fall', 'good', 'place', 'right', 'price', 'see', 'short', 'block', 'quick', 'little', 'want', 'old', 'plug', 'perfect', 'pro', 'struggle', 'needed', 'cheap'], 86: ['nice', 'galaxy', 'bought', 'cable', 'spare', 'long', 'failed', 'charge'], 87: ['better', 'start', 'galaxy', 'samson', 'charge', 'need', 'really'], 88: ['fast', 'strong', 'give', 'quality', 'charge', 'work', 'galaxy', 'first', 'long', 'well', 'great', 'love', 'mean', 'price', 'try', 'mention', 'poor', 'going', 'awesome', 'got', 'charged', 'little', 'lot', 'sure', 'cheap'], 89: ['purchase', 'galaxy', 'samson', 'back', 'charge', 'work'], 90: ['fast', 'product', 'price', 'galaxy', 'samson', 'great', 'charge', 'recommended'], 91: ['product', 'order', 'future', 'well', 'great', 'charge', 'need'], 92: ['use', 'little', 'wish', 'stress', 'break', 'reach', 'realize', 'coiled', 'big', 'charge', 'many', 'put', 'dont', 'fine', 'people'], 93: ['part', 'wrong', 'fancy', 'charge', 'receive', 'galaxy', 'return', 'purchased', 'happy', 'swooped', 'needed', 'output', 'price', 'samson', 'going', 'new', 'top', 'show', 'couple', 'received', 'go'], 94: ['price', 'bought', 'samson', 'solid', 'great', 'charge', 'perfectly'], 95: ['bought', 'back', 'pleased', 'go', 'cheap'], 96: ['used', 'like', 'wish', 'great', 'charge', 'love', 'intractable'], 97: ['connect', 'great', 'fail'], 98: ['get', 'stock', 'charge', 'work', 'last', 'great', 'big', 'say', 'name', 'fall', 'cost', 'price', 'bought', 'mention', 'pay', 'samson', 'much', 'perfectly', 'got', 'near', 'carry', 'back', 'brand', 'amazon', 'cheap'], 99: ['fast', 'use', 'good', 'lot'], 100: ['right', 'price', 'can', 'beat', 'got', 'last'], 101: ['reliable', 'adapted', 'fast', 'product', 'purchase', 'miss', 'samson', 'get', 'see', 'sit', 'defective', 'charge', 'need', 'sure', 'got'], 102: ['quickly', 'charged', 'unfordable', 'back', 'charge', 'good'], 103: ['fuse', 'attempted', 'like', 'samson', 'come', 'charge', 'unit', 'inside', 'period', 'change', 'apart'], 104: ['right', 'display', 'treat', 'price', 'green', 'shell', 'plug', 'light', 'great', 'failed', 'sum', 'failure', 'box', 'occasionally'], 105: ['bought', 'wanted', 'great', 'spring', 'work'], 106: ['couple', 'recognize', 'first', 'plugged'], 107: ['led', 'plugged', 'light', 'great', 'charge', 'fact', 'love'], 108: ['guess', 'buy', 'good', 'charge'], 109: ['quickly', 'used', 'couple', 'charge', 'good'], 110: ['brand', 'galaxy', 'samson'], 111: ['price', 'unfordable', 'needed', 'charge'], 112: ['product', 'galaxy', 'bought', 'charge', 'good', 'fine', 'perfectly'], 113: ['place', 'hold', 'buy', 'charge', 'certain'], 114: ['perfectly', 'samson', 'charge', 'sure', 'fine', 'work'], 115: ['use', 'product', 'talk', 'like', 'quality', 'past', 'expected', 'ordered', 'charge', 'fine', 'cheap'], 116: ['give', 'quality', 'return', 'past', 'take', 'well', 'great', 'waste', 'function'], 117: ['sometimes', 'light', 'use', 'hope'], 118: ['carry', 'original', 'sin', 'samson', 'cable', 'excellent'], 119: ['went', 'fast', 'rest', 'begin', 'love', 'awesome'], 120: ['bit', 'near', 'buy', 'first', 'sturdy', 'purchased', 'cable', 'force', 'much', 'well', 'charge', 'side', 'try', 'go', 'cheap'], 121: ['product', 'little', 'buy', 'quality', 'intended'], 122: ['nice', 'bought', 'long', 'great', 'put', 'rooms', 'work'], 123: ['original', 'like', 'perfect', 'charge', 'need', 'love', 'good', 'quick'], 124: ['purchased', 'guess', 'bad', 'made', 'say', 'go'], 125: ['price', 'actual', 'like', 'plug', 'really', 'disappointed', 'cap', 'charge', 'unit', 'need', 'good', 'fine', 'got', 'veryalue'], 126: ['product', 'reimbursed', 'received', 'first', 'find', 'gift', 'defective', 'much', 'manner', 'timely', 'gave', 'embarrassed'], 127: ['bought', 'back', 'point', 'spring', 'charge', 'fine', 'change'], 128: ['long', 'fast', 'well'], 129: ['product', 'came', 'like', 'look', 'samson', 'charge', 'slow', 'cheap'], 130: ['original', 'cherry', 'galaxy', 'charge', 'received', 'good', 'travel', 'perfectly'], 131: ['expensive', 'better', 'product', 'little', 'buy', 'upset', 'stop', 'samson', 'well', 'amazon', 'work'], 132: ['stay', 'square', 'charge', 'way', 'fine', 'adapted', 'shape', 'like', 'first', 'connected', 'long', 'good', 'use', 'product', 'end', 'samson', 'extra', 'certain', 'short', 'going', 'foot', 'got', 'people', 'bit', 'due', 'different', 'original', 'purpose', 'lack', 'side', 'sum', 'style', 'ordered'], 133: ['wish', 'well', 'charge', 'great', 'needed', 'cheap'], 134: ['use', 'given', 'cable', 'plain', 'sight', 'charge', 'wall'], 135: ['connect', 'galaxy', 'samson', 'screen', 'charge', 'way', 'abnormal'], 136: ['price', 'came', 'original', 'buy', 'like', 'charge', 'good'], 137: ['better', 'price', 'came', 'great', 'charge', 'good'], 138: ['wrong', 'whole', 'order', 'charge', 'adapted', 'stop', 'well', 'freeze', 'reason', 'turn', 'amazon', 'right', 'tried', 'product', 'end', 'come', 'fit', 'quick', 'wall', 'expected', 'made'], 139: ['like', 'galaxy', 'charge', 'original'], 140: ['part', 'condition', 'quality', 'get', 'charge', 'came', 'like', 'issue', 'galaxy', 'right', 'product', 'start', 'short', 'foot', 'different', 'original', 'plug', 'cable', 'note', 'amazon', 'ordered', 'cheap'], 141: ['disappoint', 'galaxy', 'surprise', 'expected', 'going', 'new', 'etc'], 142: ['quickly', 'leave', 'purchase', 'long', 'love', 'work'], 143: ['tried', 'product', 'buy', 'said', 'samson', 'make', 'well', 'charge', 'fact'], 144: ['strong', 'plug', 'charge', 'worth', 'apart'], 145: ['use', 'purchase', 'quality', 'cable', 'option', 'pleased', 'charge', 'good', 'travel'], 146: ['nice', 'like', 'promised', 'color', 'well', 'way', 'awesome'], 147: ['tell', 'ask', 'get', 'jealous', 'color', 'much', 'great', 'stylish', 'perfectly'], 148: ['charged', 'much', 'full'], 149: ['charged', 'product', 'quality', 'like', 'perfect', 'overbear', 'great', 'well', 'name', 'love', 'really', 'anywhere'], 150: ['worry', 'color', 'run'], 151: ['hold', 'bought', 'charge'], 152: ['fast', 'glossy', 'overbear', 'love', 'reliable'], 153: ['quickly', 'cool', 'color', 'well', 'attractive', 'designed'], 154: ['slid', 'surprised', 'hard', 'get', 'put', 'loved'], 155: ['strong', 'good', 'different', 'price'], 156: ['slim', 'love', 'expected', 'design'], 157: ['like', 'really', 'came', 'cut'], 158: ['white', 'enjoy', 'like', 'cut', 'show', 'piece', 'really'], 159: ['came', 'like', 'paint', 'click', 'piece', 'broken', 'snap'], 160: ['charged', 'easy', 'like', 'access', 'button', 'great', 'good'], 161: ['clear', 'back', 'great', 'muffled', 'sound'], 162: ['white', 'little', 'give', 'concerned', 'connect', 'cut', 'run', 'order', 'gave', 'go', 'black'], 163: ['price', 'like', 'doubled', 'promised', 'friend', 'great', 'beat'], 164: ['sturdy', 'purchased', 'worry', 'great', 'happy'], 165: ['plug', 'bought', 'perfect', 'get', 'color', 'shift', 'charge', 'work'], 166: ['used', 'obsessed', 'great', 'find'], 167: ['better', 'back', 'overbear', 'watch', 'great', 'good', 'fit', 'etc'], 168: ['product', 'status', 'convenient', 'like', 'back', 'great', 'charge', 'inkstand'], 169: ['price', 'give', 'many', 'good', 'etc'], 170: ['look', 'almost', 'review', 'point', 'color', 'cap', 'many', 'positive', 'amazon', 'used', 'product', 'twice', 'bought', 'see', 'multiple', 'purchase', 'overbear', 'face', 'yet', 'veryerified'], 171: ['right', 'price', 'forget', 'deal', 'charge', 'good'], 172: ['different', 'came', 'match', 'spot', 'wrong', 'bought', 'review', 'long', 'entire', 'free', 'go'], 173: ['short', 'worried'], 174: ['steady', 'way', 'twice', 'watch', 'name', 'positive', 'face'], 175: ['sleek', 'like', 'look', 'etc', 'get', 'promised', 'well', 'gas', 'needed', 'awesome', 'got'], 176: ['color', 'can', 'well', 'complain'], 177: ['stylish'], 178: ['use', 'product', 'price', 'unbearable', 'easy', 'like', 'well'], 179: ['quickly', 'extended', 'enjoy', 'buy', 'able', 'screen', 'fantastic', 'great', 'large'], 180: ['worth', 'going', 'dropped'], 181: ['weight', 'add', 'like', 'back', 'whole', 'expected', 'much', 'charge', 'got', 'last'], 182: ['great', 'mention', 'really', 'many'], 183: ['nice', 'right', 'think', 'bought', 'got', 'back', 'black', 'color', 'much', 'happy', 'save', 'good', 'excellent', 'choice'], 184: ['have', 'compare', 'excellent', 'manner'], 185: ['little', 'galaxy', 'size', 'see', 'noise', 'much', 'made', 'new', 'fine'], 186: ['right', 'bought', 'friend', 'great', 'amazon', 'made', 'fit', 'cheap'], 187: ['buy', 'nice', 'good', 'price'], 188: ['fast', 'cheap', 'overall', 'fit', 'smooth', 'purchase', 'galaxy', 'bought', 'finish', 'spring', 'happy', 'mobile', 'new', 'perfectly', 'stick'], 189: ['liked', 'bit', 'part', 'little', 'leave', 'buy', 'like', 'hard', 'purchase', 'surface', 'amount', 'well', 'single', 'table', 'good', 'hurt'], 190: ['nice', 'product', 'opening', 'notice', 'galaxy', 'bought', 'samson', 'cut', 'button', 'extra', 'small', 'bad', 'slow', 'gave', 'out', 'good', 'fit', 'reason'], 191: ['soft', 'shell', 'smooth', 'like', 'hard', 'exterior', 'great', 'say', 'feel'], 192: ['liked', 'display', 'buy', 'beat', 'get', 'find', 'screen', 'dropped', 'small', 'damage', 'wound', 'piece', 'many', 'good', 'cost', 'fit', 'reason'], 193: ['product', 'quality', 'perfect', 'great', 'inside', 'good', 'high'], 194: ['protection', 'mate', 'otterbox', 'sleek', 'prefer', 'turning', 'feel', 'style', 'ample'], 195: ['soft', 'think', 'price', 'like', 'galaxy', 'samson', 'touch', 'well', 'hit'], 196: ['work', 'expect', 'buy', 'free', 'deal', 'galaxy', 'really', 'perfect', 'pay', 'amount', 'well', 'much', 'willing', 'good', 'reason', 'period', 'initial', 'cheap'], 197: ['fast', 'trust', 'galaxy', 'look', 'samson', 'order', 'make', 'great', 'need', 'fit'], 198: ['cause', 'galaxy', 'first', 'wish', 'spring', 'read', 'work'], 199: ['went', 'want', 'galaxy', 'back', 'spring', 'great', 'sure', 'worth', 'fit'], 200: ['quality', 'galaxy', 'arrive', 'samson', 'functional', 'good', 'fit'], 201: ['used', 'bit', 'due', 'product', 'better', 'quality', 'point', 'solid', 'dropped', 'good'], 202: ['cover', 'fit', 'different', 'buy', 'galaxy', 'bought', 'size', 'found', 'design', 'compared', 'ordered'], 203: ['like', 'look', 'cut', 'good', 'cheap'], 204: ['overall', 'satisfied', 'made', 'good', 'choice'], 205: ['test', 'quality', 'veryirtually', 'plastic', 'floor', 'other', 'pattern', 'feel', 'like', 'good', 'black', 'secured', 'used', 'protection', 'taste', 'wish', 'satisfied', 'plain', 'people', 'excellent', 'quick', 'top', 'wrap', 'overall', 'purchase', 'back', 'made', 'style', 'box'], 206: ['new', 'quality', 'purchase', 'color', 'made', 'good', 'awesome'], 207: ['better', 'collect', 'down', 'well', 'design', 'feel', 'love', 'made'], 208: ['quickly', 'product', 'easy', 'get', 'etc'], 209: ['price', 'purchase', 'long', 'bad', 'great', 'slim', 'style', 'sensation', 'fit', 'etc'], 210: ['decide', 'please', 'claim', 'grade', 'need', 'okay', 'professional', 'buy', 'first', 'long', 'make', 'standard', 'model', 'used', 'someone', 'exaggerated', 'end', 'extra', 'purchase', 'cable', 'more', 'artificially', 'note', 'sure', 'major', 'transfer'], 211: ['product', 'like', 'perfect', 'cable', 'good', 'work'], 212: ['wire', 'went', 'promptly', 'people', 'first', 'pay', 'get', 'need', 'can', 'well', 'happy', 'amazon', 'come', 'say', 'change', 'last'], 213: ['like', 'complain', 'purchased', 'cable', 'extra', 'long', 'work'], 214: ['transmit', 'purchased', 'cable', 'well', 'poor', 'great'], 215: ['recognize', 'use', 'device', 'basic', 'jump', 'end', 'bought', 'able', 'cable', 'extra', 'screen', 'spring', 'charge', 'need', 'mode', 'major', 'quick', 'cheap'], 216: ['unknown', 'look', 'get', 'charge', 'work', 'device', 'purchased', 'entire', 'well', 'good', 'used', 'tried', 'bought', 'failed', 'people', 'got', 'function', 'different', 'original', 'purchase', 'cable', 'believe', 'best', 'cheap'], 217: ['use', 'price', 'easy', 'easily', 'connected', 'great', 'veryalue'], 218: ['plugged', 'better', 'original', 'cable'], 219: ['used', 'plugged', 'expect', 'work', 'wrong', 'purchase', 'bought', 'prompt', 'cable', 'much', 'great', 'charge', 'recently', 'need', 'shipped', 'yet', 'go', 'transfer'], 220: ['expensive', 'used', 'use', 'original', 'like', 'purchased', 'cable', 'touch', 'wanted', 'well', 'good', 'work'], 221: ['suggest', 'price', 'plug', 'extra', 'cable', 'whole', 'great', 'unit', 'need'], 222: ['bought', 'much', 'charge', 'say', 'perfectly', 'wall'], 223: ['charged', 'work', 'convenient', 'field', 'charge', 'couple', 'got'], 224: ['do', 'night', 'great', 'come', 'charge'], 225: ['surprised', 'charge', 'weary', 'free', 'low', 'purchased', 'well', 'book', 'cost', 'pad', 'etc', 'price', 'wall', 'due', 'charged', 'chance', 'previously', 'made', 'needed'], 226: ['completely', 'use', 'device', 'price', 'came', 'convenient', 'bought', 'full', 'able', 'long', 'enabled', 'charge', 'way', 'intended', 'great'], 227: ['better', 'tie', 'original', 'wear', 'jump', 'like', 'circle', 'general', 'cable', 'run', 'charge', 'food', 'people'], 228: ['bit', 'product', 'device', 'plugged', 'picture', 'actual', 'speed', 'log', 'etc', 'cable', 'tested', 'thick', 'result', 'shown', 'label', 'mind', 'long', 'perfectly'], 229: ['device', 'get', 'cable', 'short', 'recommended', 'really'], 230: ['expensive', 'trust', 'bad', 'etc', 'cheap'], 231: ['exact', 'leave', 'work', 'change', 'buy', 'said', 'bought', 'get', 'review', 'worry', 'expected', 'wanted', 'great', 'say', 'try', 'people', 'understand'], 232: ['charged', 'price', 'like', 'first', 'fine', 'impressed', 'charge', 'really', 'work'], 233: ['stay', 'charge', 'need', 'work', 'last', 'think', 'connected', 'well', 'use', 'plugged', 'bought', 'got', 'wall', 'original', 'want', 'hooked', 'plug', 'track', 'spare', 'transfer'], 234: ['fast', 'use', 'great', 'charge', 'work'], 235: ['product', 'price', 'like', 'extra', 'great', 'beat', 'need', 'suppose', 'veryalue'], 236: ['buy', 'quality', 'long', 'poor', 'fit'], 237: ['fit', 'use', 'transmit', 'galaxy', 'cable', 'thick', 'side', 'tip', 'fine'], 238: ['transfer'], 239: ['hold', 'plug', 'galaxy', 'samson', 'cable', 'good', 'fit'], 240: ['price', 'short', 'compared', 'good', 'veryalue'], 241: ['old', 'good', 'great', 'back'], 242: ['quality', 'hard', 'like', 'purchased', 'samson', 'get', 'many', 'amazon', 'feel', 'good', 'really', 'last'], 243: ['way', 'plugged', 'came', 'buy', 'galaxy', 'samson', 'flat', 'became', 'charge', 'find', 'surface', 'got'], 244: ['connect', 'purchased', 'samson', 'cable', 'well'], 245: ['right', 'great', 'came'], 246: ['expensive', 'work', 'like', 'wanted', 'really'], 247: ['wire', 'gauge', 'expected', 'fine'], 248: ['cable', 'boy', 'charge', 'need', 'transfer'], 249: ['used', 'move', 'fit', 'use', 'came', 'handle', 'plug', 'general', 'note', 'way', 'fine'], 250: ['due', 'better', 'work', 'galaxy', 'purchased', 'samson', 'long', 'disappointed', 'require', 'charge', 'short', 'good', 'designed', 'cheap'], 251: ['short', 'charge', 'received'], 252: ['quickly', 'perfect', 'great', 'charge'], 253: ['take', 'order', 'wanted', 'love', 'excellent'], 254: ['use', 'plugged', 'want', 'low', 'keep', 'get', 'lucky', 'well', 'charge', 'good', 'really', 'work'], 255: ['long', 'right', 'upset', 'find'], 256: ['way', 'necessary', 'quickly', 'pull', 'like', 'plug', 'charge', 'received', 'short', 'snap', 'really'], 257: ['came', 'said', 'galaxy', 'return', 'spring', 'retract', 'care', 'suppose', 'worth', 'fine', 'cheap'], 258: ['price', 'rate', 'purpose', 'charge', 'ordered', 'hit'], 259: ['price', 'guess', 'long', 'bad', 'great', 'charge', 'deal', 'last'], 260: ['worry', 'great', 'mobile', 'love', 'person', 'brief', 'travel'], 261: ['much', 'great', 'charge', 'retract', 'cost', 'fine', 'veryalue'], 262: ['refund', 'test', 'line', 'get', 'wanted', 'charge', 'retract', 'work', 'given', 'pull', 'first', 'reason', 'intractable', 'bank', 'button', 'plain', 'felt', 'best', 'tug', 'plug', 'back', 'cable', 'pulled', 'hit'], 263: ['used', 'latch', 'button', 'almost', 'got'], 264: ['give', 'long', 'satisfied', 'expected', 'reach', 'charge', 'side', 'put', 'intractable', 'got'], 265: ['price', 'concerned', 'quality', 'bought', 'extra', 'tangled', 'expected', 'great', 'love'], 266: ['product', 'device', 'thick', 'feature', 'corded', 'well', 'love', 'intractable', 'excellent'], 267: ['end', 'tight', 'get', 'charge', 'impossible', 'good', 'fine'], 268: ['fast', 'compact', 'perfect', 'shipped', 'advertise'], 269: ['gave', 'great'], 270: ['expensive', 'little', 'lot', 'much', 'well', 'charge', 'say', 'spring'], 271: ['like', 'plug', 'full', 'able', 'take', 'long', 'reach', 'night', 'charge', 'certain', 'table'], 272: ['use', 'keep', 'back', 'charge', 'good', 'travel'], 273: ['quickly', 'product', 'give', 'back', 'gift', 'pleased', 'can', 'got'], 274: ['better', 'think', 'old', 'low', 'stop', 'correct', 'worried', 'charge', 'certain', 'blow'], 275: ['great', 'first', 'purchased'], 276: ['got', 'great'], 277: ['quickly', 'went', 'take', 'learned', 'charge', 'break', 'cheap'], 278: ['do', 'quality', 'line', 'get', 'light', 'imagine', 'fine', 'red', 'stick', 'ruin', 'completely', 'dark', 'think', 'pick', 'like', 'stop', 'small', 'great', 'big', 'standard', 'black', 'use', 'mean', 'price', 'plugged', 'end', 'sometimes', 'twist', 'electrical', 'short', 'got', 'wall', 'dual', 'wise', 'cause', 'tug', 'plug', 'supply', 'cable', 'piece', 'go', 'cheap'], 279: ['little', 'get', 'wish', 'much', 'great'], 280: ['quickly', 'wire', 'hold', 'concerned', 'thick', 'worth', 'small', 'well', 'amount', 'put', 'good', 'go'], 281: ['work', 'quality', 'gift', 'brand', 'note', 'awesome', 'pad', 'quick'], 282: ['love', 'pleased', 'great', 'cheap'], 283: ['stay', 'charge', 'compared', 'work', 'anywhere', 'think', 'connected', 'able', 'great', 'love', 'expensive', 'price', 'wish', 'much', 'mobile', 'fit', 'write', 'nice', 'total', 'amazon', 'needed'], 284: ['quickly', 'buy', 'well', 'gave', 'short'], 285: ['quickly', 'satisfied', 'can', 'well', 'charge'], 286: ['purse', 'bought', 'spare', 'worry', 'many', 'work'], 287: ['nice', 'fast', 'product', 'price', 'fault', 'came', 'expect', 'perfection', 'easily', 'review', 'long', 'great', 'short', 'really'], 288: ['little', 'work', 'boost', 'extra', 'fancy', 'charge', 'needed'], 289: ['gift', 'bought', 'great', 'product'], 290: ['handy', 'use', 'plug', 'wish', 'charge', 'need', 'wheel'], 291: ['used', 'nice', 'push', 'button', 'well', 'charge', 'intractable', 'last'], 292: ['wrong', 'end', 'male', 'retract', 'pulled', 'fit'], 293: ['used', 'issue', 'well', 'yet', 'really'], 294: ['bad', 'charge', 'shortage', 'really', 'work'], 295: ['nice', 'use', 'compact', 'easy', 'like', 'long', 'great', 'way', 'intractable'], 296: ['good'], 297: ['used', 'bit', 'price', 'condition', 'lured', 'brand', 'tag', 'great', 'compared', 'new', 'excellent'], 298: ['switch', 'give', 'quality', 'sound', 'make', 'great', 'love', 'reason', 'good', 'place', 'tell', 'bought', 'feature', 'head', 'people', 'nice', 'better', 'overall', 'convenient', 'lot', 'purchase', 'attach', 'piece', 'clip', 'mouth'], 299: ['strong', 'part', 'get', 'sound', 'pick', 'able', 'decrease', 'pad', 'hold', 'output', 'hear', 'come', 'new', 'people', 'forget', 'hand', 'old', 'increase', 'best', 'occasionally']})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install require packages for OpenAI API Model"
      ],
      "metadata": {
        "id": "XA-JDBisUhQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu numpy openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhpxIa1AUlVK",
        "outputId": "85ea7ccf-895a-4e6a-d1c8-d1af70b16cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages"
      ],
      "metadata": {
        "id": "p6nhI3HAUr_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"\")\n",
        "\n",
        "# Chat completions\n",
        "def chat_completions(prompt: str):\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    ],\n",
        "    response_format={\n",
        "        \"type\": \"json_schema\",\n",
        "        \"json_schema\": {\n",
        "            \"name\": \"SentimentAnalysis\",\n",
        "            \"schema\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"sentiment\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"positive\", \"negative\"]\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"sentiment\"]\n",
        "            }\n",
        "        }\n",
        "    })\n",
        "\n",
        "  return completion.choices[0].message.content\n",
        "\n",
        "# Embeddings\n",
        "def embeddings(input: str):\n",
        "  embedding = client.embeddings.create(\n",
        "    input=input,\n",
        "    model=\"text-embedding-ada-002\"\n",
        "  )\n",
        "\n",
        "  return embedding.data[0].embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "De8WudT25QlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Faiss Index for archiving the records"
      ],
      "metadata": {
        "id": "kX42nZJoVlEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "def create_index(vector_dim):\n",
        "  index = faiss.IndexFlatL2(vector_dim)\n",
        "  return index"
      ],
      "metadata": {
        "id": "C96Bach_UwjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store the embeddings with feedback type"
      ],
      "metadata": {
        "id": "NGfIjREsVzAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Store embeddings with feedback type\n",
        "def store_embeddings(index, input_texts, feedback_types):\n",
        "    embeddings_list = []\n",
        "    for text in input_texts:\n",
        "        embeddings_list.append(embeddings(text))\n",
        "\n",
        "    embeddings_np = np.array(embeddings_list, dtype='float32')\n",
        "    index.add(embeddings_np)\n",
        "\n",
        "    #Store the feedback types alongside embeddings\n",
        "    with open('feedback_data.json', 'w') as f:\n",
        "      data = []\n",
        "      for i, feedback in enumerate(feedback_types):\n",
        "        data.append({'embedding_index': i, 'feedback_type': feedback})\n",
        "      json.dump(data, f)\n"
      ],
      "metadata": {
        "id": "N3OTyzHIUbmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Giving prompt to the OpenAI to generate the result\n"
      ],
      "metadata": {
        "id": "EX5GnA6a0rTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "role: \"you are an expert in product review classification and gives accurate result by dividing the review into positive or negative.\"                                                                                                 action: \"you need to create a table that consists of review, actual review based on the buyer thoughts it should be output either positive or negative or average, how the review is classified that is either positive, negative or average and serial numbers for all the rows\"\n",
        "context: \"If we train any model using this data it should divide the review accurately into positive or negative reviews\"\n",
        "\n",
        "** Input Data:**\n",
        "{feedback}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EcmKTgWTWKt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create index"
      ],
      "metadata": {
        "id": "xczYTBhCWTW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_dimension = 1536\n",
        "index = create_index(vector_dimension)\n"
      ],
      "metadata": {
        "id": "uyRkp4tzWVNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create a function to check if given feedback already there in the stored emeddings if not there please call chat completions with prompt and once output is extracted store it for futrue comparisiontext-embedding-ada-002\n",
        "\n",
        "def check_and_get_feedback(feedback, index):\n",
        "  \"\"\"\n",
        "  Checks if feedback exists in stored embeddings. If not, calls chat completions, stores the result, and returns it.\n",
        "  \"\"\"\n",
        "  feedback_embedding = np.array([embeddings(feedback)], dtype='float32')\n",
        "  D, I = index.search(feedback_embedding, k=1)  # Search for nearest neighbor\n",
        "\n",
        "  # Check if the nearest neighbor is close enough (adjust threshold as needed)\n",
        "  threshold = 0.1  # Example threshold, adjust based on your data\n",
        "  if D[0][0] < threshold:\n",
        "    # Feedback likely exists, retrieve it from storage\n",
        "    with open('feedback_data.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "    for item in data:\n",
        "        if item['embedding_index'] == I[0][0]:\n",
        "            print(\"Feedback found in existing embeddings.\")\n",
        "            return item['feedback_type'] # return the stored feedback type\n",
        "  else:\n",
        "    # Feedback is likely new, call chat completions\n",
        "    print(\"Calling chat completions for new feedback...\")\n",
        "    formatted_prompt = prompt.format(feedback=feedback)\n",
        "    new_feedback_type = chat_completions(formatted_prompt)\n",
        "\n",
        "    # Store the new feedback and its embedding\n",
        "    store_embeddings(index, [feedback], [new_feedback_type])\n",
        "    return new_feedback_type\n"
      ],
      "metadata": {
        "id": "VJ2ZmRgzWoaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_and_get_feedback(\"This phone is looking good!\", index)"
      ],
      "metadata": {
        "id": "RYzuh03yXZCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vENT3T3UosNP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}